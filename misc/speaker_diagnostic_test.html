<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speaker Detection Diagnostic</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        .test-section {
            background: #f5f5f5;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
        }
        .result {
            background: white;
            padding: 15px;
            margin: 10px 0;
            border-radius: 5px;
            border-left: 4px solid #007bff;
        }
        .error {
            border-left-color: #dc3545;
            background: #fff5f5;
        }
        .success {
            border-left-color: #28a745;
            background: #f5fff5;
        }
        button {
            background: #007bff;
            color: white;
            border: none;
            padding: 10px 20px;
            border-radius: 5px;
            cursor: pointer;
            margin: 5px;
        }
        button:disabled {
            background: #6c757d;
        }
        pre {
            background: #f8f9fa;
            padding: 10px;
            border-radius: 3px;
            overflow-x: auto;
            white-space: pre-wrap;
        }
    </style>
</head>
<body>
    <h1>üîç Speaker Detection Diagnostic Tool</h1>
    
    <div class="test-section">
        <h2>Step 1: Test Your Microphone</h2>
        <p>First, let's record a test audio with clear speaker separation:</p>
        <button id="recordTest" onclick="startDiagnosticRecording()">üé§ Record Test Audio (10 seconds)</button>
        <div id="recordingStatus"></div>
    </div>

    <div class="test-section">
        <h2>Step 2: Manual Speaker Test</h2>
        <p>Record yourself saying different things as "different speakers":</p>
        <ol>
            <li>Say "Hello, I am speaker one" in your normal voice</li>
            <li>Wait 2 seconds</li>
            <li>Say "Hello, I am speaker two" in a different pitch/tone</li>
            <li>Wait 2 seconds</li>
            <li>Say "This is speaker one again" in your normal voice</li>
        </ol>
        <button id="recordManual" onclick="startManualTest()">üéôÔ∏è Record Manual Test</button>
        <div id="manualStatus"></div>
    </div>

    <div class="test-section">
        <h2>Step 3: Debug Results</h2>
        <div id="debugResults"></div>
    </div>

    <script>
        let mediaRecorder = null;
        let audioChunks = [];
        let isRecording = false;

        async function startDiagnosticRecording() {
            const button = document.getElementById('recordTest');
            const status = document.getElementById('recordingStatus');
            
            try {
                const stream = await navigator.mediaDevices.getUserMedia({ 
                    audio: {
                        echoCancellation: false,  // Turn off processing that might affect speaker detection
                        noiseSuppression: false,
                        autoGainControl: false,
                        sampleRate: 44100
                    }
                });

                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                audioChunks = [];
                
                mediaRecorder.ondataavailable = function(event) {
                    if (event.data.size > 0) {
                        audioChunks.push(event.data);
                    }
                };

                mediaRecorder.onstop = function() {
                    testSpeakerDetection();
                };

                mediaRecorder.start();
                isRecording = true;
                
                button.textContent = 'Recording... (10s)';
                button.disabled = true;
                status.innerHTML = '<div class="result">üî¥ Recording in progress... Speak clearly with pauses between sentences.</div>';

                // Stop after 10 seconds
                setTimeout(() => {
                    if (isRecording) {
                        mediaRecorder.stop();
                        stream.getTracks().forEach(track => track.stop());
                        isRecording = false;
                        button.textContent = 'üé§ Record Test Audio (10 seconds)';
                        button.disabled = false;
                        status.innerHTML = '<div class="result">‚úÖ Recording complete. Processing...</div>';
                    }
                }, 10000);

            } catch (error) {
                status.innerHTML = `<div class="result error">‚ùå Error: ${error.message}</div>`;
            }
        }

        async function startManualTest() {
            const button = document.getElementById('recordManual');
            const status = document.getElementById('manualStatus');
            
            status.innerHTML = '<div class="result">üìù Follow the instructions above, then click "Record Test Audio" button.</div>';
        }

        async function testSpeakerDetection() {
            const debugResults = document.getElementById('debugResults');
            debugResults.innerHTML = '<div class="result">üîÑ Analyzing speaker detection...</div>';

            try {
                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                
                // Test 1: Send to your live-diarize endpoint
                debugResults.innerHTML += '<h3>Test 1: Live Diarization</h3>';
                const liveDiarizeResult = await testEndpoint(audioBlob, '/combined/live-diarize/');
                displayTestResult('Live Diarization', liveDiarizeResult, debugResults);
                
                // Test 2: Send to basic transcription
                debugResults.innerHTML += '<h3>Test 2: Basic Transcription (for comparison)</h3>';
                const basicResult = await testEndpoint(audioBlob, '/combined/transcribe-and-translate/');
                displayTestResult('Basic Transcription', basicResult, debugResults);
                
                // Test 3: Try to create a debug endpoint call
                debugResults.innerHTML += '<h3>Test 3: Debug Analysis</h3>';
                await createDebugAnalysis(liveDiarizeResult, basicResult, debugResults);

            } catch (error) {
                debugResults.innerHTML += `<div class="result error">‚ùå Test failed: ${error.message}</div>`;
            }
        }

        async function testEndpoint(audioBlob, endpoint) {
            const formData = new FormData();
            formData.append('file', audioBlob, 'test.webm');
            formData.append('target_language', 'vi');
            formData.append('source_language', 'auto');

            const response = await fetch(`http://localhost:8000${endpoint}`, {
                method: 'POST',
                body: formData
            });

            return await response.json();
        }

        function displayTestResult(testName, result, container) {
            let html = `<div class="result ${result.success ? 'success' : 'error'}">`;
            html += `<h4>${testName} Result:</h4>`;
            
            if (result.success) {
                if (result.speaker_segments) {
                    const speakerCount = new Set(result.speaker_segments.map(s => s.speaker)).size;
                    html += `<p><strong>Speakers detected:</strong> ${speakerCount}</p>`;
                    html += `<p><strong>Total segments:</strong> ${result.speaker_segments.length}</p>`;
                    
                    html += '<p><strong>Segments:</strong></p><ul>';
                    result.speaker_segments.forEach((segment, i) => {
                        html += `<li><strong>${segment.speaker}:</strong> "${segment.text.substring(0, 50)}..." (Method: ${segment.method || 'unknown'})</li>`;
                    });
                    html += '</ul>';
                } else if (result.transcription) {
                    html += `<p><strong>Transcription:</strong> "${result.transcription.text}"</p>`;
                    html += `<p><strong>Language detected:</strong> ${result.transcription.language}</p>`;
                }
            } else {
                html += `<p><strong>Error:</strong> ${result.error || 'Unknown error'}</p>`;
            }
            
            html += `<details><summary>Raw Response</summary><pre>${JSON.stringify(result, null, 2)}</pre></details>`;
            html += '</div>';
            
            container.innerHTML += html;
        }

        async function createDebugAnalysis(liveDiarizeResult, basicResult, container) {
            let html = '<div class="result">';
            html += '<h4>üîç Analysis & Recommendations:</h4>';
            
            // Analyze the results
            const issues = [];
            const recommendations = [];
            
            if (liveDiarizeResult.success && liveDiarizeResult.speaker_segments) {
                const speakerCount = new Set(liveDiarizeResult.speaker_segments.map(s => s.speaker)).size;
                
                if (speakerCount === 1) {
                    issues.push("Only one speaker detected despite clear voice differences");
                    recommendations.push("Audio chunks may be too short for speaker diarization");
                    recommendations.push("Try longer recording periods (15-30 seconds)");
                    recommendations.push("Ensure speakers have distinctly different vocal characteristics");
                }
                
                const fallbackMethods = liveDiarizeResult.speaker_segments.filter(s => 
                    s.method && s.method.includes('fallback')
                ).length;
                
                if (fallbackMethods === liveDiarizeResult.speaker_segments.length) {
                    issues.push("All segments using fallback methods - cloud diarization failed");
                    recommendations.push("Check AssemblyAI API key and quota");
                    recommendations.push("Audio quality may be too poor for cloud processing");
                }
            } else {
                issues.push("Live diarization completely failed");
                recommendations.push("Check server logs for detailed error messages");
                recommendations.push("Verify AssemblyAI service is working");
            }
            
            if (basicResult.success && basicResult.transcription) {
                html += `<p>‚úÖ Basic transcription works: "${basicResult.transcription.text.substring(0, 100)}..."</p>`;
            } else {
                issues.push("Basic transcription also failed");
                recommendations.push("Check your Whisper service configuration");
            }
            
            if (issues.length > 0) {
                html += '<p><strong>üö® Issues Detected:</strong></p><ul>';
                issues.forEach(issue => html += `<li>${issue}</li>`);
                html += '</ul>';
            }
            
            if (recommendations.length > 0) {
                html += '<p><strong>üí° Recommendations:</strong></p><ul>';
                recommendations.forEach(rec => html += `<li>${rec}</li>`);
                html += '</ul>';
            }
            
            html += '</div>';
            container.innerHTML += html;
        }
    </script>
</body>
</html>